<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Bridging the Bosphorus | Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Converted LaTeX to HTML</title>
    <style>
        .scriptsize { font-size: smaller; }
    </style>
</head>


<section class="hero">
<!--    <div class="hero-body">-->
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">
                        Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking</h1>
<!--                    <h3 class="title is-4 conference-authors"><a target="_blank" href="https://arxiv.org/abs/2311.01378">Arxiv Preprint</a></h3>-->
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="https://emrecanacikgoz.github.io/">Emre Can Acikgoz</a><sup>1,2</sup>,
                <a target="_blank" href="">Mete Erdogan</a><sup>1,2</sup>,
                <a target="_blank" href="https://www.denizyuret.com/">Deniz Yuret</a><sup>1,2</sup>
            </span>
                    </div>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>KoÃ§ University, KUIS AI Center, </span>
                        <span class="author-block"><sup>2</sup>KoÃ§ University, Department of Computer Engineering </span>
                    </div>

                    <div class="column has-text-centered">
                        <span class="author-block">
                            <a href="mailto:eacikgoz17@ku.edu.tr">eacikgoz17@ku.edu.tr</a></span>
                    </div>
                    <br>
                    <img src="./assets/images/bridge-3.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 70%;"/>
                    <br>
                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2405.04685"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/emrecanacikgoz/turkish-llm"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                <a target="_blank" href="https://huggingface.co/emrecanacikgoz"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">ðŸ¤—</span>
                  <span>Models</span>
                </a>
                <a target="_blank" href="https://mukayese.tdd.ai/#/tasks/mukayese_llm"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">ðŸŽ¯</span>
                  <span>Leaderboard</span>
                </a>

                <!-- Poster PDF link -->
                <span class="link-block">
                    <a href="assets/images/poster.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Poster</span>
                  </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
<!--    </div>-->
</section>



<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        Large Language Models (LLMs) are becoming crucial across various fields, emphasizing the urgency for high-quality models in underrepresented languages. This study explores the unique challenges faced by low-resource languages, such as data scarcity, model selection, evaluation, and computational limitations, with a special focus on Turkish. We conduct an in-depth analysis to evaluate the impact of training strategies, model choices, and data availability on the performance of LLMs designed for underrepresented languages. Our approach includes two methodologies: (i) adapting existing LLMs originally pretrained in English to understand Turkish, and (ii) developing a model from the ground up using Turkish pretraining data, both supplemented with supervised fine-tuning on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning capabilities. The relative performance of these methods is evaluated through the creation of a new leaderboard for Turkish LLMs, featuring benchmarks that assess different reasoning and knowledge skills. Furthermore, we conducted experiments on data and model scaling, both during pretraining and fine-tuning, simultaneously emphasizing the capacity for knowledge transfer across languages and addressing the challenges of catastrophic forgetting encountered during fine-tuning on a different language. Our goal is to offer a detailed guide for advancing the LLM framework in low-resource linguistic contexts, thereby making natural language processing (NLP) benefits more globally accessible.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column is-full">
                    <h2 class="title is-3"><span class="dvima">Advancing Turkish Large Language Models (LLMs)</span></h2>
                    <div style="font-size: 125%">
                        <p>Our contributions are as follows:</p>
                        <ul style="list-style-type: disc; margin-left: 20px;">
                            <li> <strong>Hamza LLMs.</strong> We release the Hamza LLM series: <a href="https://huggingface.co/emrecanacikgoz/hamza-small">Hamza-small</a>, <a href="https://huggingface.co/emrecanacikgoz/hamza-medium">Hamza-medium</a>, <a href="https://huggingface.co/emrecanacikgoz/hamza-large">Hamza-large</a>, and <a href="https://huggingface.co/emrecanacikgoz/hamza-xl">Hamza-xlarge</a>. Notably, Hamza-xlarge with 1.3B parameters marks the premier and most expansive open-source, scientifically vetted Turkish LLM that is trained on 300B tokens. We also introduce <a href="https://huggingface.co/emrecanacikgoz/hamza-mistral">Hamza<span class="scriptsize">Mistral</span></a> and <a href="https://huggingface.co/emrecanacikgoz/hamza-gpt2-xl">Hamza<span class="scriptsize">GPT2-xl</span></a>, adapted from Mistral 7B and GPT2-xl, respectively.</li>
                            <li> <strong>Fine-Tuning vs. From-Scratch Training.</strong> Our analysis explores two distinct methodologies for developing Turkish LLMs in resource and computational power-constrained environments: (i) extending pretrained models (Mistral-7b and GPT2-xl) with Turkish-only data (called as Hamza<span class="scriptsize">Mistral</span> and Hamza<span class="scriptsize">GPT2-xl</span>), and (ii) constructing a model from scratch, similar to the GPT2 approach. This paper thoroughly discusses the merits and drawbacks of these strategies. </li>
                            <li> <strong>Turkish LLM Benchmarking.</strong> We have curated new Turkish evaluation datasets <a href="https://huggingface.co/datasets/mukayese/truthful_qa-tr">TruthfulQA-TR</a> and <a href="https://huggingface.co/datasets/mukayese/arc-tr">ARC-TR</a> by carefully validating each with multiple annotators, offering meticulously cleaned datasets, and launching a leaderboard to catalyze ongoing advancements in Turkish LLMs. </li>
                            <li> <strong>Open Source Community.</strong> Committing to open science principles, we make all source codes, model checkpoints, and datasets open-source and publicly accessible. </li>
                        </ul>
                        <p>By detailing the development of specialized datasets and methodologies, we offer a comprehensive guide for building LLMs for languages with limited resources. Additionally, our contributions substantially enrich the field by providing critical resources that will support future research in Turkish language processing and the broader area of Natural Language Processing (NLP) for under-resourced languages.</p>
                    </div>
            </div>
        </div>
    </div>
</section>


<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Method 1: Further Training a Base Model</span></h2>
            <span style="font-size: 125%">
                We aim to enhance base LLMs with Turkish linguistic capabilities. After a detailed evaluation based on perplexity, we selected an LLM that did not specifically train on Turkish data during its initial pretraining phase. We subjected it to further training using Turkish-only data, accomplished through the next-token prediction objective implemented in an autoregressive manner. Essentially, this process can be regarded as a continuation of the pretraining phase of LLMs, but training on a specific portion of Turkish dataset this time. <br> <br>
                <span style="font-weight: bold">Selecting Base Model.</span> For the successful development of an advanced Turkish LLM with a 7 billion parameter scale, choosing the most suitable base model is essential. To this end, we have selected <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral 7B</a> as one of our base models, owing to its recent success across various tasks. Additionally, we opted for <a href="https://huggingface.co/openai-community/gpt2-xl">GPT2-xlarge</a>, since our Hamza model is trained from scratch on the <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT2</a> architecture. This selection allows for a meaningful comparison between models trained from scratch and those initially trained in English and subsequently continued with pre-training in the same architectural setup. <br> <br>
                <span style="font-weight: bold">Dataset.</span> In order to inject Turkish into Mistral and GPT-2 base LLMs, we followed a strategy of incremental continued pretraining on Turkish-specific segments of our dataset. Beginning with an initial 100MB of pure Turkish data, we progressively expanded the training corpus, culminating in the model being trained on 5GB of data. This volume aligns closely with the dataset size used for GPT, ensuring a comprehensive and effective adaptation of the model to handle Turkish linguistic nuances. <br> <br>
                <span style="font-weight: bold">Training.</span> As a continual learning approach, we conducted a series of experiments by progressively enlarging the pretraining corpus size and halting upon observing convergence. The models are initialized with the pretraining weights of the Mistral-7B and GPT2-xlarge and then further trained on segments of our text corpus with a casual language modeling objective. Throughout our continued pretraining experiments, we employed <a href="https://arxiv.org/abs/2106.09685">LoRA</a> and updated only the additional bottleneck adapter weights while freezing the original model weights to make the training cost-efficient and avoid any catastrophic forgetting from the models' previous capabilities. During our LoRA trainings, we used r=32 and alpha=32, along with a dropout rate of 0.05, applying LoRA exclusively to the projection layers. We used AdamW optimizer and cosine scheduler with a learning rate of 0.0001. Based on our experiments, we opted for a batch size of 1 and avoided gradient accumulation due to its significant impact on convergence. To simplify the execution of our experiments and ensure the reproducibility of our results, we used the <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a> repository, only in our LoRA-based continued pretraining experiments. <br> <br>
            </span>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    
                    <h2 class="title is-3"><span class="dvima">Method 2: Pretraining from Scratch</span></h2>
                    
                <span style="font-size: 125%">
                    In our final approach for developing a Turkish base-LLM, we adopted the most straightforward method: training from scratch using Turkish-only datasets. We follow a similar framework as in GPT2, with similarities in training procedures and architectural settings. However, we differed in our approach by utilizing a pretraining corpus nearly double the size of GPT2. <br> <br>
                    
                    <br>
                    <img src="assets/images/hamza.png" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 90%;"/>
                    <br>
                    
                    <span style="font-weight: bold">Pretraining Data.</span> The construction of a robust LLM hinges on the aggregation and processing of high-quality text data. To develop Hamza, we used the Turkish split of <a href="https://arxiv.org/abs/2309.09400">CulturaX</a> includes a meticulous process of data curation. It gathers a comprehensive dataset from open-sources <a href="https://huggingface.co/datasets/mc4">mC4</a> and <a href="https://oscar-project.org/">OSCAR</a>. Our pretraining data contains 128 parquet files each 1.4GB, totaling almost 179.2GB. The compiled training dataset contains 129,486,207,634 (130B) training tokens. Further details of the data gathering, structure, and preparation can be found in CulturaX work. <br> <br>
                    <span style="font-weight: bold">Architecture.</span> To develop an inaugural Turkish base model, we followed prior works, establishing a solid model for Turkish language modeling akin to earlier studies on other languages. Our approach led to the creation of four variants of Hamza, following GPT-2: hamza-small (124M parameters), hamza-medium (354M parameters), hamza-large (772M parameters), and our largest model, hamza-xlarge (1.3B parameters). The architectural specifications of these models are given in the table above. <br> <br>
                    <span style="font-weight: bold">Optimizer.</span> During our training, AdamW optimizer is used with hyper-parameters beta1=0.9 and beta2=0.95. A cosine learning rate schedule is implemented, designed to reduce the learning rate to 10% of its maximum value. Additionally, we applied a weight decay rate of 0.1 and limited the gradient norm to 1.0 to prevent overfitting. The training process includes 2,000 warm-up steps. We used a learning rate 0f 0.0006 and batch size 491,520 in our smallest model Hamza-small. We varied the learning rate and batch size according to the model size. <br> <br>
                    <span style="font-weight: bold">Training.</span>  Our from-scratch Hamza models are built on the GPT2 architecture and incorporate the flash-attention mechanism for efficient training. The hyperparameters of the model follow the scaling principles set by GPT2, except for the largest variant, Hamza-xlarge, which is inspired by a recent <a href="https://arxiv.org/abs/2402.00786">CroissantLLM</a>. All model versions were trained for 300 billion tokens, with a uniform batch size of 500,000 tokens. The learning rate was fine-tuned for each model variant. We standardized the context window across all models at 1024 tokens and did not employ any dropout techniques during their training process. All training sessions were conducted in half-precision (fp16) settings by utilizing both tensor and data parallelism across eight A100 GPUs each with 80GB of memory.
                    <br> <br>
                </span>
                </div>
            </div>

        </div>
    </div>
</section>



<!--Benchmarks-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    
                    <h2 class="title is-3"><span class="dvima">Benchmarks and Results</span></h2>
                    
                <span style="font-size: 125%">
                    Evaluating the accuracy of Turkish LLMs on various tasks can be challenging due to concerns about dataset quality. Many reasoning datasets have been directly machine-translated from English without validation, resulting in biased and inaccurate results. To address this, we introduce two Turkish datasets: <a href="https://huggingface.co/datasets/mukayese/truthful_qa-tr">TruthfulQA-TR</a>, which assesses a modelâ€™s tendency to reproduce common falsehoods, and <a href="https://huggingface.co/datasets/mukayese/arc-tr">ARC-TR</a>, a set of grade-school science questions. We used state-of-the-art tools for translation and validated the samples with multiple annotators, cleaning them as needed. We also evalute the Bits-Per-Character (BPC) rate of each model and reported them in detail. <br> <br>
                    
                    <br>
                    <img src="assets/images/results.png" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto; width: 60%;"/>
                    <br>
                    
                    <span style="font-weight: bold">Bits-Per-Character (BPC) Evaluations.</span> Auto-regressive language modeling is trained on optimizing the Negative Log-Likelihood (NLL) of the data in the training set and the effectiveness of the model is then calculated on the unseen test data. Furthermore, the most common metric to evaluate these models is perplexity, which measures the uncertainty of an LLM in predicting the next token in a sequence and is derived by taking the exponential average of the NLL. However, as various tokenizers can divide each sentence into differing numbers of tokens, NLL and PPL may produce incomparable results for models utilizing different tokenizers. To tackle this, we use Bits-Per-Character (BPC), which is another critical metric derived from NLL, used for evaluating the performance of LLMs at character-level. Consequently, our comparisons mainly relied on BPC, which normalizes the impact of tokenization differences. For the BPC evaluation, we utilized the test set of the <a href="https://github.com/tdd-ai/trnews-64/">trnews-64</a> corpus, comprising 5,000 samples. <br> <br>
                    <span style="font-weight: bold">Prompting and Few-Shot.</span> Evaluating the reasoning capabilities of LLMs in downstream Question Answering (QA) tasks is crucial to assess their abilities. However, finding such datasets in languages other than English is challenging due to the limited availability of benchmarks in these languages. To bridge this gap, we developed TruthfulQA-TR and ARC-TR Turkish question-answering datasets, that are designed to evaluate the ability of LLMs to generate truthful and accurate responses to questions. To develop the Turkish version of the main <a href="https://arxiv.org/abs/2109.07958">TruthfulQA Multiple Choice (MC)</a> dataset and <a href="https://arxiv.org/abs/1803.05457">ARC (AI2 Reasoning Challenge)</a> dataset, we translated each example of these datasets using the advanced <a href="https://www.deepl.com">DeepL Machine Translation (MT)</a> framework by its <a href="https://github.com/DeepLcom/deepl-python">Python-supported API</a>. After translating to Turkish, each sample was reviewed for errors or superficial translations. We used the test sets from TruthfulQA-MC2 and ARC-Challenge for evaluations. Our experiments followed the same prompting settings with <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM-Leaderboard</a>. <br> <br>
                    <span style="font-weight: bold">Results.</span> We present the BPC results of different models evaluated on trnews-64 in the last column of resuts table above; including our models together with various open-source multi-lingual and Turkish LLMs. Looking at the BPC results, we observe a wide range of values across the models. Lower BPC values indicate better performance in terms of compression, suggesting that the model is more efficient in representing the text. The most favorable outcomes are attained with the pretrained <a href="https://huggingface.co/asafaya/kanarya-2b">Kanarya-2b</a> and <a href="https://huggingface.co/emrecanacikgoz/hamza-xl">Hamza-xlarge</a> models. The adapted models which are originally pretrained on English but extended to Turkish, yielded promising results as well, lower than 1 BPC, whereas the multilingual models had a relatively lower performance. Our Prompting and Few-shot evaluations were conducted on the newly established Turkish Benchmarks, ARC-TR, in 25-shot settings, as well as on TruthfulQA-TR, adhering to the same settings as outlined by the LLM-Leaderboard. In ARC-TR, Google's <a href="https://huggingface.co/google/gemma-7b">Gemma 7B</a> model leads with an accuracy of 46.16 even though it is not specifically tuned for Turkish, closely followed by <a href="https://huggingface.co/sambanovasystems/SambaLingo-Turkish-Chat">Sambalingo-tr</a> with 44.37 accuracy. Moreover, in the TruthfulQA-TR evaluation, <a href="https://huggingface.co/Trendyol/Trendyol-LLM-7b-chat-dpo-v1.0">Trendyol's DPO model</a> emerges as the top performer with an accuracy of 50.11, while <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">Mistral-7b-chat-v2</a> secures the second position with 48.34 accuracy. The accuracy scores for ARC-TR range from 24 to 47, and for TruthfulQA-TR, from 33 to 50. These results underscore the necessity for substantial improvements in these models to reach the proficiency levels observed in English benchmarks.
                </span>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Case Studies</span></h2>
                    <p style="font-size: 125%">
                        We investigate three main questions during experiments:
                        <ul style="font-size: 125%; padding-left: 5%">
                            
                        <li><span style="font-weight: bold;">(i) Enhancing Non-English Models: Fine-Tuning vs. From-Scratch Training. </span> 
                            The analysis of Turkish language models, specifically comparing models trained from scratch, continued pretraining from GPT2-xl, and those adapted using Mistral 7B, shows insightful trends. According to table, the Mistral 7B adapted model exhibits superior performance on Turkish question-answering tasks, compared to other methods. Moreover, starting from scratch surpasses the continued pretraining approach within the same model architecture, underscoring the significance of the base language model when undertaking continued pretraining. This is evidenced by the discrepancy in accuracy between models fine-tuned from Mistral 7B versus those from GPT2. Therefore, applying continued pretraining to a robust base language model emerges as the most effective strategy for low-resource languages, considering both data scarcity and hardware constraints. </li>
                            <br>
                            <img src="assets/images/case1.png" class="interpolation-image"
                            alt="" style="display: block; margin-left: auto; margin-right: auto; width: 45%;"/>
                            <br>
                        
                            
                        <li><span style="font-weight: bold;">(ii) Effect of Supervised Fine-Tuning: Assessing Model Performance with the Proposed IT Dataset.</span>
                            Supervised Fine-Tuning (SFT) plays a crucial role in enhancing the reasoning capabilities of LLMs, 
                            as highlighted in existing research. In this context, we introduced a novel Turkish 
                            IT Dataset, meticulously crafted from the ground up, inspired by the Self-Insturct and Alpaca. 
                            By fine-tuning our largest model Hamza-xlarge with this bespoke Turkish IT Dataset, we observed an improvement
                            in model performance across downstream benchmarks. 
                             This improvement underscores the effectiveness of SFT when applied to our tailored IT dataset, 
                             bolstering our model's reasoning proficiency slightly.
                            </li> 
                            <br>
                            <img src="assets/images/case2.png" class="interpolation-image"
                            alt="" style="display: block; margin-left: auto; margin-right: auto; width: 45%;"/>
                            <br>

                            
                        <li><span style="font-weight: bold;">(iii) Retention after Fine-Tuning: Will Models Forget English-Learned Skills When Fine-Tuning on Another Language?</span>
                            According to table and plots, further pretraining of base English language models such as GPT2 and Mistral results in a decrease in accuracy proportional to the number of samples used during continued pretraining on the English downstream tasks TruthfulQA and ARC, compared to their original base scores before fine-tuning on Turkish. This indicates catastrophic forgetting, where the models lose their prior knowledge upon being fine-tuned on a smaller language dataset, as evidenced by a decline in baseline accuracy compared to the versions not previously trained, even after applying techniques like LoRA training. One further work for this could be including some English data along with Turkish in each batch during continued pretraining.</li>
                            <br>
                            <img src="assets/images/case3.png" class="interpolation-image"
                            alt="" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/>
                            <br>
                    </ul>
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">

        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>
                    <div style="font-size: 125%;">
                        <span style="font-size: 100%;">
                            Our work advances the development of Turkish LLMs, presenting a new series of models both trained from scratch (Hamza) and also adapted from other base LLMs (Hamza<span class="scriptsize">Mistral</span> and Hamza<span class="scriptsize">GPT2-xl</span>), together with new Instruction Tuning dataset and a meticulously crafted Turkish LLM Leaderboard. In our analysis, we noted that the base LLMs exhibited catastrophic forgetting of their primary language knowledge during continued pretraining. Additionally, through the creation of a novel Turkish LLM evaluation benchmark, we have identified a significant performance gap between current Turkish LLMs and their English counterparts, underscoring the need for further improvements in Turkish language modeling. Our fully open-source work and detailed observations plays a pivotal role in the field of Turkish language modeling, providing insights on construction methodologies and offering a comparative framework for evaluating performance, thereby paving the way for future advancements.
                        </span>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
        @misc{acikgoz2024bridging,
            title={Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking}, 
            author={Emre Can Acikgoz and Mete Erdogan and Deniz Yuret},
            year={2024},
            eprint={2405.04685},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
        }
        </code></pre>
    </div>
</section>


<section class="section" id="Acknowledgements">
    <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgements</h2>
        This work is supported in part provided by the KUIS AI Center. The numerical calculations reported in this paper were fully/partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources). Last but not least, we also acknowledge VSB â€“ Technical University of Ostrava, IT4Innovations National Supercomputing Center, Czech Republic, for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (grant ID: 90254)
    </div>
  </section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column">
                <div class="content has-text-centered">
                    <p>
                        Website template borrowed from <a
                            href="https://vimalabs.github.io/">VIMA</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
